{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to XGBoost Spark with GPU\n",
    "\n",
    "Taxi is an example of xgboost regressor. In this notebook, we will show you how to load data, train the xgboost model and use this model to predict \"fare_amount\" of your taxi trip. Comparing to original XGBoost Spark codes, there're only two API differences.\n",
    "\n",
    "\n",
    "## Load libraries\n",
    "First we load some common libraries that both GPU version and CPU version xgboost will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.{XGBoostRegressor, XGBoostRegressionModel}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is new to xgboost-spark users is only `rapids.GpuDataReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.rapids.{GpuDataReader, GpuDataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries needed for CPU version are not needed in GPU version any more. The extra libraries needed for CPU are like below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.FloatType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainPath = /data/taxi/csv/train/\n",
       "trainWithEvalPath = /data/taxi/csv/trainWithEval/\n",
       "evalPath = /data/taxi/csv/eval/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/taxi/csv/eval/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set the paths of datasets for training and prediction\n",
    "// You need to update them to your real paths!\n",
    "val trainPath = \"/data/taxi/csv/train/\"\n",
    "val trainWithEvalPath = \"/data/taxi/csv/trainWithEval/\"\n",
    "val evalPath  = \"/data/taxi/csv/eval/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the schema of the dataset\n",
    "For Taxi example, the data has 16 columns: 15 features and 1 label. \"fare_amount\" is set to the label column. The schema will be used to help load data in the future. We also defined some key parameters used in xgboost training process. We also set some basic xgboost parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = <lazy>\n",
       "labelName = fare_amount\n",
       "paramMap = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy val schema =\n",
    "  StructType(Array(\n",
    "    StructField(\"vendor_id\", DoubleType),\n",
    "    StructField(\"passenger_count\", DoubleType),\n",
    "    StructField(\"trip_distance\", DoubleType),\n",
    "    StructField(\"pickup_longitude\", DoubleType),\n",
    "    StructField(\"pickup_latitude\", DoubleType),\n",
    "    StructField(\"rate_code\", DoubleType),\n",
    "    StructField(\"store_and_fwd\", DoubleType),\n",
    "    StructField(\"dropoff_longitude\", DoubleType),\n",
    "    StructField(\"dropoff_latitude\", DoubleType),\n",
    "    StructField(labelName, DoubleType),\n",
    "    StructField(\"hour\", DoubleType),\n",
    "    StructField(\"year\", IntegerType),\n",
    "    StructField(\"month\", IntegerType),\n",
    "    StructField(\"day\", DoubleType),\n",
    "    StructField(\"day_of_week\", DoubleType),\n",
    "    StructField(\"is_weekend\", DoubleType)\n",
    "  ))\n",
    "\n",
    "val labelName = \"fare_amount\"\n",
    "\n",
    "lazy val paramMap = Map(\n",
    "  \"learning_rate\" -> 0.05,\n",
    "  \"max_depth\" -> 8,\n",
    "  \"subsample\" -> 0.8,\n",
    "  \"gamma\" -> 1,\n",
    "  \"num_round\" -> 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new spark session and load data\n",
    "we must create a new spark session to continue all spark operations. It will also be used to initilize the `GpuDataReader` which is a data reader powered by GPU.\n",
    "\n",
    "NOTE: in this notebook, we have uploaded dependency jars when installing toree kernel. If we don't upload them at installation time, we can also upload in notebook by [%AddJar magic](https://toree.incubator.apache.org/docs/current/user/faq/). However, there's one restriction for `%AddJar`: the jar uploaded can only be available when `AddJar` is called after a new spark session is created. We must use it as below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"Taxi-GPU\").getOrCreate\n",
    "%AddJar file:/data/libs/cudf-0.9-cuda10.jar\n",
    "%AddJar file:/data/libs/xgboost4j_2.11-1.0.0-Beta.jar\n",
    "%AddJar file:/data/libs/xgboost4j-spark_2.11-1.0.0-Beta.jar\n",
    "// ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@27d062ec\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@27d062ec"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"Taxi-GPU\").getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first API difference, we now use GpuDataReader to load dataset. Similar to original Spark data loading API, GpuDataReader also uses chaining call of \"option\", \"schema\",\"csv\". For CPU verions data reader, the code is like below:\n",
    "\n",
    "```scala\n",
    "val dataReader = spark.read\n",
    "```\n",
    "\n",
    "`featureNames` is used to tell xgboost which columns are `features`, and which column is `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reader = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataReader@57c9d28a\n",
       "featureNames = List(vendor_id, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code, store_and_fwd, dropoff_longitude, dropoff_latitude, hour, year, month, day, day_of_week, is_weekend)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(vendor_id, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code, store_and_fwd, dropoff_longitude, dropoff_latitude, hour, year, month, day, day_of_week, is_weekend)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reader = new GpuDataReader(spark).option(\"header\", true).schema(schema)\n",
    "val featureNames = schema.filter(_.name != labelName).map(_.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize XGBoostRegressor\n",
    "The second API difference is `setFeaturesCol` in CPU version vs `setFeaturesCols` in GPU version. setFeaturesCol accepts a String that indicates which vectorized column is the feature column. It requires `VectorAssembler` to help vectorize all feature columns into one. setFeaturesCols accepts a list of strings so that we don't need VectorAssembler any more. So GPU verion help reduce the preparation codes before you train your xgboost model.\n",
    "\n",
    "CPU version:\n",
    "```scala\n",
    "object Vectorize {\n",
    "  def apply(df: DataFrame, featureNames: Seq[String], labelName: String): DataFrame = {\n",
    "    val toFloat = df.schema.map(f => col(f.name).cast(FloatType))\n",
    "    new VectorAssembler()\n",
    "      .setInputCols(featureNames.toArray)\n",
    "      .setOutputCol(\"features\")\n",
    "      .transform(df.select(toFloat:_*))\n",
    "      .select(col(\"features\"), col(labelName))\n",
    "  }\n",
    "}\n",
    "val reader = spark.read.option(\"header\", true).schema(schema)\n",
    "var trainSet = reader.csv(trainPath)\n",
    "var evalSet = reader.csv(evalPath)\n",
    "trainSet = Vectorize(trainSet, featureNames, labelColName)\n",
    "evalSet = Vectorize(evalSet, featureNames, labelColName)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While with GpuDataReader, `VectorAssembler` is not needed any more. We can simply read data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@40ff4c27\n",
       "trainWithEvalSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@be125b7\n",
       "evalSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@2b60bd9a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@2b60bd9a"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainSet = reader.csv(trainPath)\n",
    "val trainWithEvalSet = reader.csv(trainWithEvalPath)\n",
    "val evalSet = reader.csv(evalPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add XGBoost parameters for GPU version\n",
    "Another modification is `num_workers` should be set to the number of machines with GPU in Spark cluster, while it can be set to the number of your CPU cores in CPU version\n",
    "```scala\n",
    "// difference in parameters\n",
    "\"tree_method\" -> \"hist\",\n",
    "\"num_workers\" -> 12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbParamFinal = Map(learning_rate -> 0.05, num_workers -> 1, subsample -> 0.8, max_depth -> 8, num_round -> 500, tree_method -> gpu_hist, gamma -> 1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(learning_rate -> 0.05, num_workers -> 1, subsample -> 0.8, max_depth -> 8, num_round -> 500, tree_method -> gpu_hist, gamma -> 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xgbParamFinal = paramMap ++ Map(\"tree_method\" -> \"gpu_hist\", \"num_workers\" -> 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize XGBoostRegressor\n",
    "The second API difference is `setFeaturesCol` in CPU version vs `setFeaturesCols` in GPU version. `setFeaturesCol` accepts a String that indicates which vectorized column is the feature column. It requires `VectorAssembler` to help vectorize all feature columns into one. setFeaturesCols accepts a list of strings so that we don't need VectorAssembler any more. So GPU verion help reduce the preparation codes before you train your xgboost model.\n",
    "\n",
    "CPU version:\n",
    "```scala\n",
    "val xgbRegressor = new XGBoostRegressor(xgbParamFinal)\n",
    "      .setLabelCol(labelColName)\n",
    "      .setFeaturesCol(\"features\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbRegressor = xgbr_2eee9ab3afd6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "xgbr_2eee9ab3afd6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xgbRegressor = new XGBoostRegressor(xgbParamFinal)\n",
    "  .setLabelCol(labelName)\n",
    "  .setFeaturesCols(featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark and train\n",
    "The benchmark object is for calculating training time. We will use it to compare with xgboost in CPU version.\n",
    "\n",
    "We also support training with evaluation sets in 2 ways as same as CPU version behavior:\n",
    "\n",
    "* API `setEvalSets` after initializing an XGBoostClassifier\n",
    "\n",
    "```scala\n",
    "xgbClassifier.setEvalSets(Map(\"eval\" -> evalSet))\n",
    "\n",
    "```\n",
    "\n",
    "* parameter `eval_sets` when initializing an XGBoostClassifier\n",
    "\n",
    "```scala\n",
    "val paramMapWithEval = paramMap + (\"eval_sets\" -> Map(\"eval\" -> evalSet))\n",
    "val xgbClassifierWithEval = new XGBoostClassifier(paramMapWithEval)\n",
    "```\n",
    "\n",
    "in this notebook, we use API method to set evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbr_2eee9ab3afd6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbRegressor.setEvalSets(Map(\"eval\" -> trainWithEvalSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.124, DMLC_TRACKER_PORT=9092, DMLC_NUM_WORKER=1}\n",
      "Elapsed time [train]: 14.501s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object Benchmark\n",
       "model = xgbr_2eee9ab3afd6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "xgbr_2eee9ab3afd6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Benchmark {\n",
    "  def time[R](phase: String)(block: => R): (R, Float) = {\n",
    "    val t0 = System.currentTimeMillis\n",
    "    val result = block // call-by-name\n",
    "    val t1 = System.currentTimeMillis\n",
    "    println(\"Elapsed time [\" + phase + \"]: \" + ((t1 - t0).toFloat / 1000) + \"s\")\n",
    "    (result, (t1 - t0).toFloat / 1000)\n",
    "  }\n",
    "}\n",
    "\n",
    "// start training\n",
    "val (model, _) = Benchmark.time(\"train\") {\n",
    "  xgbRegressor.fit(trainSet)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation and evaluation\n",
    "We use `evalSet` to evaluate our model and use some key columns to show our predictions. Finally we use `RegressionEvaluator` to calculate an overall accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [transform]: 2.416s\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "|    vendor_id|passenger_count|trip_distance|fare_amount|        prediction|\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "| 1.55973043E9|            1.0|          2.3|        7.7| 7.165289402008057|\n",
      "| 1.55973043E9|            1.0|          1.7|        8.5| 7.477686405181885|\n",
      "|-1.67996288E9|            2.0|          5.0|       13.3|13.023439407348633|\n",
      "| 1.55973043E9|            1.0|          3.0|       11.7|11.613130569458008|\n",
      "| 1.55973043E9|            2.0|          1.2|        5.8| 6.099410533905029|\n",
      "| 1.55973043E9|            1.0|          1.7|        7.4|  7.53718376159668|\n",
      "| 1.55973043E9|            1.0|          5.1|       15.4|14.448943138122559|\n",
      "| 1.55973043E9|            1.0|          0.4|        4.6| 4.475068092346191|\n",
      "| 1.55973043E9|            2.0|          2.1|        8.2| 8.562113761901855|\n",
      "| 1.55973043E9|            1.0|          1.0|        4.9| 5.467292308807373|\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Elapsed time [evaluation]: 0.229s\n",
      "RMSE == 0.6899989190851109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prediction = [vendor_id: float, passenger_count: float ... 15 more fields]\n",
       "evaluator = regEval_e3a5b4f23a9f\n",
       "rmse = 0.6899989190851109\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6899989190851109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// start transform\n",
    "val (prediction, _) = Benchmark.time(\"transform\") {\n",
    "  val ret = model.transform(evalSet).cache()\n",
    "  ret.foreachPartition(_ => ())\n",
    "  ret\n",
    "}\n",
    "prediction.select(\"vendor_id\", \"passenger_count\", \"trip_distance\", labelName, \"prediction\").show(10)\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(labelName)\n",
    "val (rmse, _) = Benchmark.time(\"evaluation\") {\n",
    "  evaluator.evaluate(prediction)\n",
    "}\n",
    "println(s\"RMSE == $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to disk and load model\n",
    "We save the model to disk and then load it to memory. We can use the loaded model to do a new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [transform2]: 0.044s\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "|    vendor_id|passenger_count|trip_distance|fare_amount|        prediction|\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "| 1.55973043E9|            1.0|          2.3|        7.7| 7.165289402008057|\n",
      "| 1.55973043E9|            1.0|          1.7|        8.5| 7.477686405181885|\n",
      "|-1.67996288E9|            2.0|          5.0|       13.3|13.023439407348633|\n",
      "| 1.55973043E9|            1.0|          3.0|       11.7|11.613130569458008|\n",
      "| 1.55973043E9|            2.0|          1.2|        5.8| 6.099410533905029|\n",
      "+-------------+---------------+-------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modelFromDisk = xgbr_2eee9ab3afd6\n",
       "results2 = [vendor_id: float, passenger_count: float ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[vendor_id: float, passenger_count: float ... 15 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.write.overwrite.save(\"/data/model/taxi\")\n",
    "\n",
    "val modelFromDisk = XGBoostRegressionModel.load(\"/data/model/taxi\")\n",
    "val (results2, _) = Benchmark.time(\"transform2\") {\n",
    "  modelFromDisk.transform(evalSet)\n",
    "}\n",
    "results2.select(\"vendor_id\", \"passenger_count\", \"trip_distance\", labelName, \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0904 - Scala",
   "language": "scala",
   "name": "0904_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
