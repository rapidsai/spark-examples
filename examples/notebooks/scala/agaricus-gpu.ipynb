{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to XGBoost Spark with GPU\n",
    "\n",
    "Agaricus is an example of xgboost classifier. In this notebook, we will show you how to load data, train the xgboost model and use this model to predict if a mushroom is \"poisonous\". Camparing to original XGBoost Spark codes, there're only two API differences.\n",
    "\n",
    "## Load libraries\n",
    "First we load some common libraries that both GPU version and CPU version xgboost will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.{XGBoostClassifier, XGBoostClassificationModel}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is new to xgboost-spark users is only `rapids.GpuDataReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries needed for original CPU version are not needed in GPU version any more. In CPU version, we need to import extra libraries like below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.FloatType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainPath = /data/agaricus/csv/train/\n",
       "trainWithEvalPath = /data/agaricus/csv/trainWithEval/\n",
       "evalPath = /data/agaricus/csv/eval/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/agaricus/csv/eval/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set the paths of datasets for training and prediction\n",
    "// You need to update them to your real paths!\n",
    "val trainPath = \"/data/agaricus/csv/train/\"\n",
    "val trainWithEvalPath = \"/data/agaricus/csv/trainWithEval/\"\n",
    "val evalPath  = \"/data/agaricus/csv/eval/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the schema of the dataset\n",
    "\n",
    "for agaricus example, the data has 126 dimensions and we name them as \"feature_0\", \"feature_1\" ... \"feature_126\". The schema will be used to help load data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelName = label\n",
       "dataSchema = StructType(StructField(label,DoubleType,true), StructField(feature_0,DoubleType,true), StructField(feature_1,DoubleType,true), StructField(feature_2,DoubleType,true), StructField(feature_3,DoubleType,true), StructField(feature_4,DoubleType,true), StructField(feature_5,DoubleType,true), StructField(feature_6,DoubleType,true), StructField(feature_7,DoubleType,true), StructField(feature_8,DoubleType,true), StructField(feature_9,DoubleType,true), StructField(feature_10,DoubleType,true), StructField(feature_11,DoubleType,true), StructField(feature_12,DoubleType,true), StructField(feature_13,DoubleType,true), StructFie...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "featureNames: (length: Int)List[String]\n",
       "schema: (length: Int)org.apache.spark.sql.types.StructType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(label,DoubleType,true), StructField(feature_0,DoubleType,true), StructField(feature_1,DoubleType,true), StructField(feature_2,DoubleType,true), StructField(feature_3,DoubleType,true), StructField(feature_4,DoubleType,true), StructField(feature_5,DoubleType,true), StructField(feature_6,DoubleType,true), StructField(feature_7,DoubleType,true), StructField(feature_8,DoubleType,true), StructField(feature_9,DoubleType,true), StructField(feature_10,DoubleType,true), StructField(feature_11,DoubleType,true), StructField(feature_12,DoubleType,true), StructField(feature_13,DoubleType,true), StructFie..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelName = \"label\"\n",
    "def featureNames(length: Int): List[String] =\n",
    "  0.until(length).map(i => s\"feature_$i\").toList.+:(labelName)\n",
    "\n",
    "def schema(length: Int): StructType =\n",
    "  StructType(featureNames(length).map(n => StructField(n, DoubleType)))\n",
    "\n",
    "val dataSchema = schema(126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new spark session and load data\n",
    "\n",
    "We must create a new spark session to continue all spark operations. It will also be used to initilize the `GpuDataReader` which is a data reader powered by GPU.\n",
    "\n",
    "NOTE: in this notebook, we have uploaded dependency jars when installing toree kernel. If we don't upload them at installation time, we can also upload in notebook by [%AddJar magic](https://toree.incubator.apache.org/docs/current/user/faq/). However, there's one restriction for `%AddJar`: the jar uploaded can only be available when `AddJar` is called after a new spark session is created. We must use it as below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"agaricus-GPU\").getOrCreate\n",
    "%AddJar file:/data/libs/cudf-0.9.1-cuda10.jar\n",
    "%AddJar file:/data/libs/xgboost4j_2.11-1.0.0-Beta2.jar\n",
    "%AddJar file:/data/libs/xgboost4j-spark_2.11-1.0.0-Beta2.jar\n",
    "// ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@42fadb6c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@42fadb6c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// build spark session\n",
    "val spark = SparkSession.builder.appName(\"agaricus-gpu\").getOrCreate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first API difference, we now use `GpuDataReader` to load dataset. Similar to original Spark data loading API, `GpuDataReader` also uses chaining call of \"option\", \"schema\",\"csv\". For `CPU` verion data reader, the code is like below:\n",
    "\n",
    "```scala\n",
    "val dataReader = spark.read\n",
    "```\n",
    "\n",
    "`featureCols` is used to tell xgboost which columns are `feature` and while column is `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataReader = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataReader@21f1947a\n",
       "featureCols = List(feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9, feature_10, feature_11, feature_12, feature_13, feature_14, feature_15, feature_16, feature_17, feature_18, feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_27, feature_28, feature_29, feature_30, feature_31, feature_32, feature_33, feature_34, feature_35, feature_36, feature_37, feature_38, feature_39, feature_40, feature_41, feature_42, feature_43, feature_44, feature_45, feature_46, feature_47, feature_48, feature_49, feature_50, feature_51, feature_52, feature_53, fe...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9, feature_10, feature_11, feature_12, feature_13, feature_14, feature_15, feature_16, feature_17, feature_18, feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_27, feature_28, feature_29, feature_30, feature_31, feature_32, feature_33, feature_34, feature_35, feature_36, feature_37, feature_38, feature_39, feature_40, feature_41, feature_42, feature_43, feature_44, feature_45, feature_46, feature_47, feature_48, feature_49, feature_50, feature_51, feature_52, feature_53, fe..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// build data reader\n",
    "val dataReader = new GpuDataReader(spark)\n",
    "val featureCols = dataSchema.filter(_.name != labelName).map(_.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `dataReader` to read data directly. However, in CPU version, we have to use `VectorAssembler` to assemble all feature columns into one column. The reason will be explained later. the CPU version code is as below:\n",
    "\n",
    "```scala\n",
    "object Vectorize {\n",
    "  def apply(df: DataFrame, featureNames: Seq[String], labelName: String): DataFrame = {\n",
    "    val toFloat = df.schema.map(f => col(f.name).cast(FloatType))\n",
    "    new VectorAssembler()\n",
    "      .setInputCols(featureNames.toArray)\n",
    "      .setOutputCol(\"features\")\n",
    "      .transform(df.select(toFloat:_*))\n",
    "      .select(col(\"features\"), col(labelName))\n",
    "  }\n",
    "}\n",
    "\n",
    "val trainSet = reader.csv(trainPath)\n",
    "val evalSet = reader.csv(evalPath)\n",
    "trainSet = Vectorize(trainSet, featureCols, labelName)\n",
    "evalSet = Vectorize(evalSet, featureCols, labelName)\n",
    "\n",
    "```\n",
    "\n",
    "While with GpuDataReader, `VectorAssembler` is not needed any more. We can simply read data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@c35dcf0\n",
       "trainWithEvalSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@468d0483\n",
       "evalSet = ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@6e9204c7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ml.dmlc.xgboost4j.scala.spark.rapids.GpuDataset@6e9204c7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load data of training and evaluation\n",
    "var (trainSet, trainWithEvalSet, evalSet) = {\n",
    "  dataReader.option(\"header\", true).schema(dataSchema)\n",
    "  (dataReader.csv(trainPath), dataReader.csv(trainWithEvalPath),dataReader.csv(evalPath))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set xgboost parameters and initilize XGBoostClassifier\n",
    "\n",
    "The only difference here is `num_workers` should be set to the number of machines with GPU in Spark cluster, while it can be set to the number of your CPU cores in CPU version:\n",
    "\n",
    "```scala\n",
    "// difference in parameters\n",
    "  \"num_workers\" -> 12,\n",
    "  \"tree_method\" -> \"hist\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap = Map(num_workers -> 1, max_depth -> 2, num_round -> 100, missing -> 0.0, tree_method -> gpu_hist, eta -> 0.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(num_workers -> 1, max_depth -> 2, num_round -> 100, missing -> 0.0, tree_method -> gpu_hist, eta -> 0.1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// build XGBoost classifier\n",
    "val paramMap = Map(\n",
    "  \"eta\" -> 0.1,\n",
    "  \"max_depth\" -> 2,\n",
    "  \"num_workers\" -> 1,\n",
    "  \"tree_method\" -> \"gpu_hist\",\n",
    "  \"missing\" -> 0.0,\n",
    "  \"num_round\" -> 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second API difference is `setFeaturesCol` in CPU version vs `setFeaturesCols` in GPU version. In previous blocks, we said that CPU version need `VectorAssembler` to assemble all feature columns, the reason is: `setFeaturesCol` accepts a String that indicates which vectorized column is the `feature` column. It requires `VectorAssembler` to help vectorize all feature columns into one. However, `setFeaturesCols` accepts a list of strings so that we don't need `VectorAssembler` any more. \n",
    "\n",
    "CPU version:\n",
    "\n",
    "```scala\n",
    "val xgbClassifier  = new XGBoostClassifier(paramMap)\n",
    "  .setLabelCol(labelName)\n",
    "  .setFeaturesCol(\"features\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbClassifier = xgbc_00511571b736\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "xgbc_00511571b736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xgbClassifier  = new XGBoostClassifier(paramMap)\n",
    "  .setLabelCol(labelName)\n",
    "  // === diff ===\n",
    "  .setFeaturesCols(featureCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark and train\n",
    "The benchmark object is for calculating training time. We will use it to compare with xgboost in CPU version.\n",
    "\n",
    "We also support training with evaluation sets in 2 ways as same as CPU version behavior:\n",
    "\n",
    "* API `setEvalSets` after initializing an XGBoostClassifier\n",
    "\n",
    "```scala\n",
    "xgbClassifier.setEvalSets(Map(\"eval\" -> evalSet))\n",
    "\n",
    "```\n",
    "\n",
    "* parameter `eval_sets` when initializing an XGBoostClassifier\n",
    "\n",
    "```scala\n",
    "val paramMapWithEval = paramMap + (\"eval_sets\" -> Map(\"eval\" -> evalSet))\n",
    "val xgbClassifierWithEval = new XGBoostClassifier(paramMapWithEval)\n",
    "```\n",
    "\n",
    "in this notebook, we use API method to set evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbc_00511571b736"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbClassifier.setEvalSets(Map(\"eval\" -> trainWithEvalSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Training ------\n",
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.19.183.124, DMLC_TRACKER_PORT=9092, DMLC_NUM_WORKER=1}\n",
      "Elapsed time [train]: 8.776s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object Benchmark\n",
       "xgbClassificationModel = xgbc_00511571b736\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "xgbc_00511571b736"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Benchmark {\n",
    "  def time[R](phase: String)(block: => R): (R, Float) = {\n",
    "    val t0 = System.currentTimeMillis\n",
    "    val result = block // call-by-name\n",
    "    val t1 = System.currentTimeMillis\n",
    "    println(\"Elapsed time [\" + phase + \"]: \" + ((t1 - t0).toFloat / 1000) + \"s\")\n",
    "    (result, (t1 - t0).toFloat / 1000)\n",
    "  }\n",
    "}\n",
    "\n",
    "// start training\n",
    "println(\"\\n------ Training ------\")\n",
    "val (xgbClassificationModel, _) = Benchmark.time(\"train\") {\n",
    "  xgbClassifier.fit(trainSet)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation and evaluation\n",
    "We use `evalSet` to evaluate our model and use some key columns to show our predictions. Finally we use `MulticlassClassificationEvaluator` to calculate an overall accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Transforming ------\n",
      "Elapsed time [transform]: 2.593s\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0080436170101...|[0.99195638298988...|       0.0|\n",
      "|  0.0|[-0.0080436170101...|[0.99195638298988...|       0.0|\n",
      "|  0.0|[-0.1416745483875...|[0.85832545161247...|       0.0|\n",
      "|  0.0|[-0.0747678875923...|[0.92523211240768...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0145334601402...|[0.98546653985977...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0457237958908...|[0.95427620410919...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "------Accuracy of Evaluation------\n",
      "accuracy == 0.9987577063864658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results = [label: float, feature_0: float ... 128 more fields]\n",
       "evaluator = mcEval_d5e223cecfdc\n",
       "accuracy = 0.9987577063864658\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9987577063864658"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// start transform\n",
    "println(\"\\n------ Transforming ------\")\n",
    "val (results, _) = Benchmark.time(\"transform\") {\n",
    "  val ret = xgbClassificationModel.transform(evalSet).cache()\n",
    "  ret.foreachPartition(_ => ())\n",
    "  ret\n",
    "}\n",
    "results.select(labelName, \"rawPrediction\", \"probability\", \"prediction\").show(10)\n",
    "\n",
    "println(\"\\n------Accuracy of Evaluation------\")\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelName)\n",
    "val accuracy = evaluator.evaluate(results)\n",
    "\n",
    "println(s\"accuracy == $accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to disk and load model\n",
    "We save the model to disk and then load it to memory. We can use the loaded model to do a new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [transform2]: 0.053s\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0080436170101...|[0.99195638298988...|       0.0|\n",
      "|  0.0|[-0.0080436170101...|[0.99195638298988...|       0.0|\n",
      "|  0.0|[-0.1416745483875...|[0.85832545161247...|       0.0|\n",
      "|  0.0|[-0.0747678875923...|[0.92523211240768...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0145334601402...|[0.98546653985977...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "|  0.0|[-0.0457237958908...|[0.95427620410919...|       0.0|\n",
      "|  1.0|[-0.9667758941650...|[0.03322410583496...|       1.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modelFromDisk = xgbc_00511571b736\n",
       "results2 = [label: float, feature_0: float ... 128 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: float, feature_0: float ... 128 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbClassificationModel.write.overwrite.save(\"/data/model/agaricus\")\n",
    "\n",
    "val modelFromDisk = XGBoostClassificationModel.load(\"/data/model/agaricus\")\n",
    "val (results2, _) = Benchmark.time(\"transform2\") {\n",
    "  modelFromDisk.transform(evalSet)\n",
    "}\n",
    "results2.select(labelName, \"rawPrediction\", \"probability\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0904 - Scala",
   "language": "scala",
   "name": "0904_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
